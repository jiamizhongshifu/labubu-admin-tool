//
//  VisionService.swift
//  jitata
//
//  Created by é’Ÿåº†æ ‡ on 2025/6/7.
//

import Foundation
import UIKit
import VisionKit
@preconcurrency import Vision
import CoreImage.CIFilterBuiltins
import ImageIO

@MainActor
class VisionService: ObservableObject {
    
    static let shared = VisionService()
    
    private init() {}
    
    /// ä½¿ç”¨VisionKitç§»é™¤èƒŒæ™¯
    func removeBackground(from image: UIImage) async throws -> UIImage {
        // é¦–å…ˆä¿®æ­£å›¾åƒæ–¹å‘
        let orientedImage = fixImageOrientation(image)
        print("ğŸš€ å¼€å§‹å¤„ç†å›¾åƒï¼ŒåŸå§‹å°ºå¯¸: \(image.size)ï¼Œä¿®æ­£åå°ºå¯¸: \(orientedImage.size)")
        
        guard let inputImage = CIImage(image: orientedImage) else {
            print("âŒ å›¾åƒè½¬æ¢å¤±è´¥")
            throw VisionError.invalidImage
        }
        
        print("ğŸ“ CIImage extent: \(inputImage.extent)")
        
        // ä½¿ç”¨Visionæ¡†æ¶çš„ä¸»ä½“åˆ†ç¦»åŠŸèƒ½ï¼ˆiOS 17+æ¨èæ–¹æ³•ï¼‰
        if #available(iOS 17.0, *) {
            do {
                let result = try await performVisionSubjectLifting(image: orientedImage)
                print("âœ… Visionä¸»ä½“åˆ†ç¦»æˆåŠŸï¼Œç»“æœå°ºå¯¸: \(result.size)")
                return result
            } catch {
                print("âš ï¸ Visionä¸»ä½“åˆ†ç¦»å¤±è´¥: \(error)ï¼Œå°è¯•é™çº§æ–¹æ¡ˆ")
                // é™çº§åˆ°å…¶ä»–æ–¹æ³•
            }
        }
        
        // é™çº§æ–¹æ¡ˆï¼šä½¿ç”¨ä¼ ç»ŸVisionæŠ€æœ¯
        do {
            let result = try await performAdvancedVisionProcessing(ciImage: inputImage)
            print("âœ… é™çº§å¤„ç†æˆåŠŸï¼Œç»“æœå°ºå¯¸: \(result.size)")
            return result
        } catch {
            print("âŒ æ‰€æœ‰å¤„ç†æ–¹æ³•éƒ½å¤±è´¥: \(error)")
            throw error
        }
    }
    
    /// é«˜çº§Visionå¤„ç†ï¼ˆä¸“ä¸ºæ½®ç©ç‰©å“ä¼˜åŒ–ï¼‰
    private func performAdvancedVisionProcessing(ciImage: CIImage) async throws -> UIImage {
        print("ğŸ”„ å¼€å§‹é«˜çº§Visionå¤„ç†...")
        
        // ç­–ç•¥1: é¦–å…ˆå°è¯•VisionKitä¸»ä½“åˆ†ç¦»ï¼ˆiOS 17+ï¼‰
        if #available(iOS 17.0, *) {
            do {
                print("ğŸ¯ å°è¯•VisionKitä¸»ä½“åˆ†ç¦»...")
                let originalImage = UIImage(ciImage: ciImage) ?? UIImage()
                let result = try await performVisionSubjectLifting(image: originalImage)
                print("âœ… VisionKitä¸»ä½“åˆ†ç¦»æˆåŠŸ")
                return result
            } catch {
                print("âš ï¸ VisionKitä¸»ä½“åˆ†ç¦»å¤±è´¥: \(error)")
            }
        }
        
        // ç­–ç•¥2: å°è¯•äººåƒåˆ†å‰²ï¼ˆé€‚ç”¨äºäººå½¢æ‰‹åŠï¼‰
        do {
            print("ğŸ‘¤ å°è¯•äººåƒåˆ†å‰²...")
            let result = try await performPersonSegmentation(ciImage: ciImage)
            print("âœ… äººåƒåˆ†å‰²æˆåŠŸ")
            return result
        } catch {
            print("âš ï¸ äººåƒåˆ†å‰²å¤±è´¥: \(error)")
        }
        
        // ç­–ç•¥3: å°è¯•æ˜¾è‘—æ€§æ£€æµ‹ï¼ˆé€‚ç”¨äºå¤§å¤šæ•°çªå‡ºç‰©ä½“ï¼‰
        do {
            print("ğŸ¯ å°è¯•æ˜¾è‘—æ€§æ£€æµ‹...")
            let result = try await performEnhancedSaliencyDetection(ciImage: ciImage)
            print("âœ… æ˜¾è‘—æ€§æ£€æµ‹æˆåŠŸ")
            return result
        } catch {
            print("âš ï¸ æ˜¾è‘—æ€§æ£€æµ‹å¤±è´¥: \(error)")
        }
        
        // ç­–ç•¥4: ä½¿ç”¨æ”¹è¿›çš„è¾¹ç¼˜æ£€æµ‹ï¼ˆæœ€åçš„é™çº§æ–¹æ¡ˆï¼‰
        print("ğŸ”§ ä½¿ç”¨æ”¹è¿›çš„è¾¹ç¼˜æ£€æµ‹...")
        do {
            let result = try await performEnhancedEdgeDetection(ciImage: ciImage)
            print("âœ… è¾¹ç¼˜æ£€æµ‹æˆåŠŸ")
            return result
        } catch {
            print("âŒ æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥äº†: \(error)")
            throw error
        }
    }
    

    
    /// é«˜çº§ç‰©ä½“åˆ†å‰²ï¼ˆæ”¯æŒå„ç§ç‰©ä½“ç±»å‹ï¼‰
    @available(iOS 17.0, *)
    private func performAdvancedObjectSegmentation(ciImage: CIImage, originalImage: UIImage) async throws -> UIImage {
        // å°è¯•å¤šç§åˆ†å‰²æ–¹æ³•ï¼Œä¼˜å…ˆçº§ä»é«˜åˆ°ä½
        
        // 1. é¦–å…ˆå°è¯•äººåƒåˆ†å‰²ï¼ˆå¯¹äººç‰©æœ€å‡†ç¡®ï¼‰
        do {
            return try await performPersonSegmentation(ciImage: ciImage)
        } catch {
            print("äººåƒåˆ†å‰²å¤±è´¥: \(error)")
        }
        
        // 2. å°è¯•ç‰©ä½“æ£€æµ‹å’Œåˆ†å‰²
        do {
            return try await performObjectDetectionSegmentation(ciImage: ciImage)
        } catch {
            print("ç‰©ä½“æ£€æµ‹åˆ†å‰²å¤±è´¥: \(error)")
        }
        
        // 3. ä½¿ç”¨æ˜¾è‘—æ€§æ£€æµ‹ä½œä¸ºæœ€åçš„é™çº§æ–¹æ¡ˆ
        return try await performEnhancedSaliencyDetection(ciImage: ciImage)
    }
    
    /// ä½¿ç”¨ç‰©ä½“æ£€æµ‹è¿›è¡Œåˆ†å‰²
    @available(iOS 17.0, *)
    private func performObjectDetectionSegmentation(ciImage: CIImage) async throws -> UIImage {
        return try await withCheckedThrowingContinuation { continuation in
            // ä½¿ç”¨çŸ©å½¢æ£€æµ‹æ¥è¯†åˆ«ä¸»è¦ç‰©ä½“åŒºåŸŸ
            let request = VNDetectRectanglesRequest { request, error in
                if let error = error {
                    continuation.resume(throwing: error)
                    return
                }
                
                guard let observations = request.results as? [VNRectangleObservation],
                      !observations.isEmpty else {
                    // å¦‚æœçŸ©å½¢æ£€æµ‹å¤±è´¥ï¼Œé™çº§åˆ°æ˜¾è‘—æ€§æ£€æµ‹
                    Task {
                        do {
                            let result = try await self.performEnhancedSaliencyDetection(ciImage: ciImage)
                            continuation.resume(returning: result)
                        } catch {
                            continuation.resume(throwing: error)
                        }
                    }
                    return
                }
                
                do {
                    // æ‰¾åˆ°ç½®ä¿¡åº¦æœ€é«˜çš„çŸ©å½¢
                    let bestObservation = observations.max { $0.confidence < $1.confidence }
                    guard let mainRect = bestObservation else {
                        continuation.resume(throwing: VisionError.noResults)
                        return
                    }
                    
                    // åŸºäºæ£€æµ‹åˆ°çš„çŸ©å½¢åŒºåŸŸåˆ›å»ºè’™ç‰ˆ
                    let maskedImage = try self.createMaskFromBoundingBox(
                        ciImage: ciImage,
                        boundingBox: mainRect.boundingBox
                    )
                    continuation.resume(returning: maskedImage)
                } catch {
                    continuation.resume(throwing: error)
                }
            }
            
            // è®¾ç½®çŸ©å½¢æ£€æµ‹å‚æ•°
            request.minimumAspectRatio = 0.1
            request.maximumAspectRatio = 10.0
            request.minimumSize = 0.1
            request.maximumObservations = 5
            
            // æ‰§è¡ŒçŸ©å½¢æ£€æµ‹
            let handler = VNImageRequestHandler(ciImage: ciImage, options: [:])
            do {
                try handler.perform([request])
            } catch {
                continuation.resume(throwing: error)
            }
        }
    }
    
    /// åŸºäºè¾¹ç•Œæ¡†åˆ›å»ºè’™ç‰ˆ
    private func createMaskFromBoundingBox(ciImage: CIImage, boundingBox: CGRect) throws -> UIImage {
        let imageSize = ciImage.extent.size
        
        // è½¬æ¢Visionåæ ‡ç³»åˆ°Core Imageåæ ‡ç³»
        let convertedBox = CGRect(
            x: boundingBox.minX * imageSize.width,
            y: (1 - boundingBox.maxY) * imageSize.height,
            width: boundingBox.width * imageSize.width,
            height: boundingBox.height * imageSize.height
        )
        
        // åˆ›å»ºç™½è‰²è’™ç‰ˆ
        guard let maskFilter = CIFilter(name: "CIConstantColorGenerator") else {
            throw VisionError.filterCreationFailed
        }
        maskFilter.setValue(CIColor.white, forKey: kCIInputColorKey)
        
        guard let whiteMask = maskFilter.outputImage?.cropped(to: convertedBox) else {
            throw VisionError.filterProcessingFailed
        }
        
        // åˆ›å»ºé»‘è‰²èƒŒæ™¯
        guard let blackFilter = CIFilter(name: "CIConstantColorGenerator") else {
            throw VisionError.filterCreationFailed
        }
        blackFilter.setValue(CIColor.black, forKey: kCIInputColorKey)
        
        guard let blackBackground = blackFilter.outputImage?.cropped(to: ciImage.extent) else {
            throw VisionError.filterProcessingFailed
        }
        
        // åˆæˆæœ€ç»ˆè’™ç‰ˆ
        guard let compositeFilter = CIFilter(name: "CISourceOverCompositing") else {
            throw VisionError.filterCreationFailed
        }
        compositeFilter.setValue(whiteMask, forKey: kCIInputImageKey)
        compositeFilter.setValue(blackBackground, forKey: kCIInputBackgroundImageKey)
        
        guard let finalMask = compositeFilter.outputImage else {
            throw VisionError.filterProcessingFailed
        }
        
        // åº”ç”¨è’™ç‰ˆåˆ°åŸå›¾
        guard let maskedFilter = CIFilter(name: "CIBlendWithMask") else {
            throw VisionError.filterCreationFailed
        }
        maskedFilter.setValue(ciImage, forKey: kCIInputImageKey)
        maskedFilter.setValue(finalMask, forKey: kCIInputMaskImageKey)
        maskedFilter.setValue(CIImage.empty(), forKey: kCIInputBackgroundImageKey)
        
        guard let result = maskedFilter.outputImage,
              let cgImage = CIContext().createCGImage(result, from: result.extent) else {
            throw VisionError.imageConversionFailed
        }
        
        return UIImage(cgImage: cgImage)
    }
    
    /// ä½¿ç”¨Core Imageæå–ä¸»ä½“ï¼ˆé€‚ç”¨äºå„ç§ç‰©ä½“ï¼‰
    private func extractSubjectWithCoreImage(from image: CIImage) -> CIImage {
        // å¤šæ­¥éª¤å¤„ç†æ¥æå–ä¸»ä½“
        
        // 1. é¦–å…ˆå¢å¼ºå¯¹æ¯”åº¦ï¼Œçªå‡ºä¸»ä½“
        guard let contrastFilter = CIFilter(name: "CIColorControls") else {
            return image
        }
        contrastFilter.setValue(image, forKey: kCIInputImageKey)
        contrastFilter.setValue(1.2, forKey: kCIInputContrastKey) // å¢åŠ å¯¹æ¯”åº¦
        contrastFilter.setValue(1.1, forKey: kCIInputSaturationKey) // å¢åŠ é¥±å’Œåº¦
        
        guard let enhancedImage = contrastFilter.outputImage else {
            return image
        }
        
        // 2. ä½¿ç”¨è¾¹ç¼˜æ£€æµ‹æ‰¾åˆ°ç‰©ä½“è½®å»“
        guard let edgeFilter = CIFilter(name: "CIEdges") else {
            return enhancedImage
        }
        edgeFilter.setValue(enhancedImage, forKey: kCIInputImageKey)
        edgeFilter.setValue(2.0, forKey: kCIInputIntensityKey) // å¢å¼ºè¾¹ç¼˜æ£€æµ‹
        
        guard let edges = edgeFilter.outputImage else {
            return enhancedImage
        }
        
        // 3. ä½¿ç”¨å½¢æ€å­¦æ“ä½œå¡«å……è¾¹ç¼˜å†…éƒ¨
        guard let morphologyFilter = CIFilter(name: "CIMorphologyGradient") else {
            return enhancedImage
        }
        morphologyFilter.setValue(edges, forKey: kCIInputImageKey)
        morphologyFilter.setValue(3.0, forKey: kCIInputRadiusKey)
        
        guard let morphed = morphologyFilter.outputImage else {
            return enhancedImage
        }
        
        // 4. åˆ›å»ºè’™ç‰ˆå¹¶åº”ç”¨åˆ°åŸå›¾
        guard let maskFilter = CIFilter(name: "CIColorInvert") else {
            return enhancedImage
        }
        maskFilter.setValue(morphed, forKey: kCIInputImageKey)
        
        guard let mask = maskFilter.outputImage else {
            return enhancedImage
        }
        
        // 5. ä½¿ç”¨è’™ç‰ˆæ··åˆåŸå›¾å’Œé€æ˜èƒŒæ™¯
        guard let blendFilter = CIFilter(name: "CIBlendWithMask") else {
            return enhancedImage
        }
        blendFilter.setValue(image, forKey: kCIInputImageKey)
        blendFilter.setValue(mask, forKey: kCIInputMaskImageKey)
        blendFilter.setValue(CIImage.empty(), forKey: kCIInputBackgroundImageKey)
        
        return blendFilter.outputImage ?? enhancedImage
    }
    
    /// é€šç”¨ç‰©ä½“åˆ†å‰²ï¼ˆé€‚ç”¨äºiOS 15+ï¼‰
    private func performGeneralObjectSegmentation(ciImage: CIImage) async throws -> UIImage {
        // å¯¹äºiOS 15+ï¼Œå°è¯•å¤šç§åˆ†å‰²æ–¹æ³•
        
        // 1. é¦–å…ˆå°è¯•äººåƒåˆ†å‰²ï¼ˆå¯¹äººç‰©æœ€å‡†ç¡®ï¼‰
        do {
            return try await performPersonSegmentation(ciImage: ciImage)
        } catch {
            print("äººåƒåˆ†å‰²å¤±è´¥: \(error)")
        }
        
        // 2. å°è¯•æ˜¾è‘—æ€§æ£€æµ‹ï¼ˆé€‚ç”¨äºå„ç§çªå‡ºç‰©ä½“ï¼‰
        do {
            return try await performEnhancedSaliencyDetection(ciImage: ciImage)
        } catch {
            print("æ˜¾è‘—æ€§æ£€æµ‹å¤±è´¥: \(error)")
        }
        
        // 3. æœ€åä½¿ç”¨åŸºç¡€çš„è¾¹ç¼˜æ£€æµ‹æ–¹æ³•
        return try await performBasicEdgeDetectionSegmentation(ciImage: ciImage)
    }
    
    /// åŸºç¡€è¾¹ç¼˜æ£€æµ‹åˆ†å‰²ï¼ˆæœ€åçš„é™çº§æ–¹æ¡ˆï¼‰
    private func performBasicEdgeDetectionSegmentation(ciImage: CIImage) async throws -> UIImage {
        // ä½¿ç”¨Core Imageçš„è¾¹ç¼˜æ£€æµ‹å’Œå½¢æ€å­¦æ“ä½œ
        let processedImage = extractSubjectWithCoreImage(from: ciImage)
        
        guard let cgImage = CIContext().createCGImage(processedImage, from: processedImage.extent) else {
            throw VisionError.imageConversionFailed
        }
        
        return UIImage(cgImage: cgImage)
    }
    
    /// ä½¿ç”¨äººåƒåˆ†å‰²è¿›è¡ŒèƒŒæ™¯ç§»é™¤
    private func performPersonSegmentation(ciImage: CIImage) async throws -> UIImage {
        return try await withCheckedThrowingContinuation { continuation in
            // ä½¿ç”¨äººåƒåˆ†å‰²è¯·æ±‚
            let request = VNGeneratePersonSegmentationRequest { request, error in
                if let error = error {
                    continuation.resume(throwing: error)
                    return
                }
                
                guard let observation = request.results?.first as? VNPixelBufferObservation else {
                    continuation.resume(throwing: VisionError.noResults)
                    return
                }
                
                do {
                    let maskedImage = try self.applyPersonSegmentationMask(
                        to: ciImage,
                        maskObservation: observation
                    )
                    continuation.resume(returning: maskedImage)
                } catch {
                    continuation.resume(throwing: error)
                }
            }
            
            // è®¾ç½®è¯·æ±‚è´¨é‡
            request.qualityLevel = .accurate
            request.outputPixelFormat = kCVPixelFormatType_OneComponent8
            
            // æ‰§è¡Œè¯·æ±‚
            let handler = VNImageRequestHandler(ciImage: ciImage, options: [:])
            do {
                try handler.perform([request])
            } catch {
                continuation.resume(throwing: error)
            }
        }
    }
    
    /// å¢å¼ºçš„æ˜¾è‘—æ€§æ£€æµ‹ï¼ˆä¸“ä¸ºæ½®ç©ä¼˜åŒ–ï¼‰
    private func performEnhancedSaliencyDetection(ciImage: CIImage) async throws -> UIImage {
        return try await withCheckedThrowingContinuation { continuation in
            // ä½¿ç”¨æ˜¾è‘—æ€§æ£€æµ‹æ¥è¯†åˆ«å‰æ™¯å¯¹è±¡
            let request = VNGenerateAttentionBasedSaliencyImageRequest { request, error in
                if let error = error {
                    continuation.resume(throwing: error)
                    return
                }
                
                guard let observation = request.results?.first as? VNSaliencyImageObservation else {
                    continuation.resume(throwing: VisionError.noResults)
                    return
                }
                
                do {
                    let maskedImage = try self.applyEnhancedSaliencyMask(
                        to: ciImage,
                        saliencyObservation: observation
                    )
                    continuation.resume(returning: maskedImage)
                } catch {
                    continuation.resume(throwing: error)
                }
            }
            
            // æ‰§è¡Œè¯·æ±‚
            let handler = VNImageRequestHandler(ciImage: ciImage, options: [:])
            do {
                try handler.perform([request])
            } catch {
                continuation.resume(throwing: error)
            }
        }
    }
    
    /// å¢å¼ºçš„è¾¹ç¼˜æ£€æµ‹ï¼ˆæœ€åçš„é™çº§æ–¹æ¡ˆï¼‰
    private func performEnhancedEdgeDetection(ciImage: CIImage) async throws -> UIImage {
        // ä½¿ç”¨å¤šç§æŠ€æœ¯ç»„åˆæ¥æå–ä¸»ä½“
        let processedImage = extractSubjectWithAdvancedCoreImage(from: ciImage)
        
        guard let cgImage = CIContext().createCGImage(processedImage, from: processedImage.extent) else {
            throw VisionError.imageConversionFailed
        }
        
        return UIImage(cgImage: cgImage)
    }
    
    /// åº”ç”¨äººåƒåˆ†å‰²è’™ç‰ˆåˆ°å›¾åƒ
    private func applyPersonSegmentationMask(to image: CIImage, maskObservation: VNPixelBufferObservation) throws -> UIImage {
        let maskImage = CIImage(cvPixelBuffer: maskObservation.pixelBuffer)
        
        // è°ƒæ•´è’™ç‰ˆå°ºå¯¸ä»¥åŒ¹é…åŸå›¾åƒ
        let scaleX = image.extent.width / maskImage.extent.width
        let scaleY = image.extent.height / maskImage.extent.height
        let scaledMask = maskImage.transformed(by: CGAffineTransform(scaleX: scaleX, y: scaleY))
        
        // åˆ›å»ºé€æ˜èƒŒæ™¯
        let transparentBackground = CIImage(color: CIColor.clear).cropped(to: image.extent)
        
        // ä½¿ç”¨è’™ç‰ˆæ··åˆ
        guard let blendFilter = CIFilter(name: "CIBlendWithMask") else {
            throw VisionError.filterCreationFailed
        }
        
        blendFilter.setValue(image, forKey: kCIInputImageKey)
        blendFilter.setValue(transparentBackground, forKey: kCIInputBackgroundImageKey)
        blendFilter.setValue(scaledMask, forKey: kCIInputMaskImageKey)
        
        guard let outputImage = blendFilter.outputImage else {
            throw VisionError.filterProcessingFailed
        }
        
        // è½¬æ¢ä¸ºUIImage
        let context = CIContext()
        guard let cgImage = context.createCGImage(outputImage, from: outputImage.extent) else {
            throw VisionError.imageConversionFailed
        }
        
        return UIImage(cgImage: cgImage)
    }
    
    /// åº”ç”¨å¢å¼ºçš„æ˜¾è‘—æ€§è’™ç‰ˆï¼ˆä¸“ä¸ºæ½®ç©ä¼˜åŒ–ï¼‰
    private func applyEnhancedSaliencyMask(to image: CIImage, saliencyObservation: VNSaliencyImageObservation) throws -> UIImage {
        let saliencyImage = CIImage(cvPixelBuffer: saliencyObservation.pixelBuffer)
        
        // è°ƒæ•´æ˜¾è‘—æ€§å›¾åƒçš„å°ºå¯¸ä»¥åŒ¹é…åŸå›¾åƒ
        let scaledSaliency = saliencyImage.transformed(by: CGAffineTransform(
            scaleX: image.extent.width / saliencyImage.extent.width,
            y: image.extent.height / saliencyImage.extent.height
        ))
        
        // å¤šæ­¥éª¤å¢å¼ºè’™ç‰ˆè´¨é‡
        
        // 1. å¢å¼ºå¯¹æ¯”åº¦å’Œäº®åº¦
        guard let contrastFilter = CIFilter(name: "CIColorControls") else {
            throw VisionError.filterCreationFailed
        }
        contrastFilter.setValue(scaledSaliency, forKey: kCIInputImageKey)
        contrastFilter.setValue(3.0, forKey: kCIInputContrastKey) // å¤§å¹…å¢åŠ å¯¹æ¯”åº¦
        contrastFilter.setValue(0.2, forKey: kCIInputBrightnessKey) // è°ƒæ•´äº®åº¦
        
        guard let enhancedMask = contrastFilter.outputImage else {
            throw VisionError.filterProcessingFailed
        }
        
        // 2. ä½¿ç”¨å½¢æ€å­¦æ“ä½œå¡«å……ç©ºæ´
        guard let morphologyFilter = CIFilter(name: "CIMorphologyGradient") else {
            throw VisionError.filterCreationFailed
        }
        morphologyFilter.setValue(enhancedMask, forKey: kCIInputImageKey)
        morphologyFilter.setValue(2.0, forKey: kCIInputRadiusKey)
        
        guard let morphedMask = morphologyFilter.outputImage else {
            throw VisionError.filterProcessingFailed
        }
        
        // 3. è½»å¾®æ¨¡ç³Šä»¥å¹³æ»‘è¾¹ç¼˜
        guard let blurFilter = CIFilter(name: "CIGaussianBlur") else {
            throw VisionError.filterCreationFailed
        }
        blurFilter.setValue(morphedMask, forKey: kCIInputImageKey)
        blurFilter.setValue(1.0, forKey: kCIInputRadiusKey)
        
        guard let smoothMask = blurFilter.outputImage else {
            throw VisionError.filterProcessingFailed
        }
        
        // 4. ä½¿ç”¨è’™ç‰ˆæ··åˆ
        guard let blendFilter = CIFilter(name: "CIBlendWithMask") else {
            throw VisionError.filterCreationFailed
        }
        
        // åˆ›å»ºé€æ˜èƒŒæ™¯
        let transparentBackground = CIImage(color: CIColor.clear)
            .cropped(to: image.extent)
        
        blendFilter.setValue(image, forKey: kCIInputImageKey)
        blendFilter.setValue(transparentBackground, forKey: kCIInputBackgroundImageKey)
        blendFilter.setValue(smoothMask, forKey: kCIInputMaskImageKey)
        
        guard let outputImage = blendFilter.outputImage else {
            throw VisionError.filterProcessingFailed
        }
        
        // è½¬æ¢ä¸ºUIImage
        let context = CIContext()
        guard let cgImage = context.createCGImage(outputImage, from: outputImage.extent) else {
            throw VisionError.imageConversionFailed
        }
        
        return UIImage(cgImage: cgImage)
    }
    
    /// ä½¿ç”¨é«˜çº§Core ImageæŠ€æœ¯æå–ä¸»ä½“ï¼ˆä¸“ä¸ºæ½®ç©ä¼˜åŒ–ï¼‰
    private func extractSubjectWithAdvancedCoreImage(from image: CIImage) -> CIImage {
        print("ä½¿ç”¨é«˜çº§Core Imageå¤„ç†...")
        
        // 1. é¢„å¤„ç†ï¼šå¢å¼ºå›¾åƒè´¨é‡
        guard let enhancedImage = preprocessImageForSegmentation(image) else {
            return image
        }
        
        // 2. å¤šç§è¾¹ç¼˜æ£€æµ‹æŠ€æœ¯ç»„åˆ
        guard let combinedEdges = detectCombinedEdges(enhancedImage) else {
            return enhancedImage
        }
        
        // 3. åˆ›å»ºæ™ºèƒ½è’™ç‰ˆ
        guard let smartMask = createSmartMask(from: combinedEdges, originalImage: image) else {
            return enhancedImage
        }
        
        // 4. åº”ç”¨è’™ç‰ˆå¹¶ä¼˜åŒ–ç»“æœ
        return applyMaskWithOptimization(mask: smartMask, to: image)
    }
    
    /// é¢„å¤„ç†å›¾åƒä»¥ä¼˜åŒ–åˆ†å‰²æ•ˆæœ
    private func preprocessImageForSegmentation(_ image: CIImage) -> CIImage? {
        // 1. å¢å¼ºå¯¹æ¯”åº¦å’Œé¥±å’Œåº¦
        guard let contrastFilter = CIFilter(name: "CIColorControls") else { return nil }
        contrastFilter.setValue(image, forKey: kCIInputImageKey)
        contrastFilter.setValue(1.3, forKey: kCIInputContrastKey)
        contrastFilter.setValue(1.2, forKey: kCIInputSaturationKey)
        contrastFilter.setValue(0.05, forKey: kCIInputBrightnessKey)
        
        guard let enhanced = contrastFilter.outputImage else { return nil }
        
        // 2. è½»å¾®é”åŒ–ä»¥çªå‡ºç»†èŠ‚
        guard let sharpenFilter = CIFilter(name: "CISharpenLuminance") else { return enhanced }
        sharpenFilter.setValue(enhanced, forKey: kCIInputImageKey)
        sharpenFilter.setValue(0.4, forKey: kCIInputSharpnessKey)
        
        return sharpenFilter.outputImage ?? enhanced
    }
    
    /// ç»„åˆå¤šç§è¾¹ç¼˜æ£€æµ‹æŠ€æœ¯
    private func detectCombinedEdges(_ image: CIImage) -> CIImage? {
        // 1. æ ‡å‡†è¾¹ç¼˜æ£€æµ‹
        guard let edgeFilter = CIFilter(name: "CIEdges") else { return nil }
        edgeFilter.setValue(image, forKey: kCIInputImageKey)
        edgeFilter.setValue(1.5, forKey: kCIInputIntensityKey)
        
        guard let edges1 = edgeFilter.outputImage else { return nil }
        
        // 2. çº¿æ¡æ£€æµ‹
        guard let lineFilter = CIFilter(name: "CILineOverlay") else { return edges1 }
        lineFilter.setValue(image, forKey: kCIInputImageKey)
        lineFilter.setValue(0.1, forKey: "inputNRNoiseLevel")
        lineFilter.setValue(0.7, forKey: "inputNRSharpness")
        lineFilter.setValue(0.08, forKey: "inputEdgeIntensity")
        lineFilter.setValue(0.5, forKey: "inputThreshold")
        lineFilter.setValue(3.0, forKey: "inputContrast")
        
        guard let edges2 = lineFilter.outputImage else { return edges1 }
        
        // 3. ç»„åˆä¸¤ç§è¾¹ç¼˜æ£€æµ‹ç»“æœ
        guard let combineFilter = CIFilter(name: "CIAdditionCompositing") else { return edges1 }
        combineFilter.setValue(edges1, forKey: kCIInputImageKey)
        combineFilter.setValue(edges2, forKey: kCIInputBackgroundImageKey)
        
        return combineFilter.outputImage ?? edges1
    }
    
    /// åˆ›å»ºæ™ºèƒ½è’™ç‰ˆ
    private func createSmartMask(from edges: CIImage, originalImage: CIImage) -> CIImage? {
        // 1. å½¢æ€å­¦æ“ä½œå¡«å……è¾¹ç¼˜
        guard let morphologyFilter = CIFilter(name: "CIMorphologyGradient") else { return nil }
        morphologyFilter.setValue(edges, forKey: kCIInputImageKey)
        morphologyFilter.setValue(4.0, forKey: kCIInputRadiusKey)
        
        guard let filled = morphologyFilter.outputImage else { return nil }
        
        // 2. åè½¬é¢œè‰²åˆ›å»ºè’™ç‰ˆ
        guard let invertFilter = CIFilter(name: "CIColorInvert") else { return filled }
        invertFilter.setValue(filled, forKey: kCIInputImageKey)
        
        guard let inverted = invertFilter.outputImage else { return filled }
        
        // 3. æ¨¡ç³Šè’™ç‰ˆè¾¹ç¼˜ä»¥è·å¾—æ›´è‡ªç„¶çš„æ•ˆæœ
        guard let blurFilter = CIFilter(name: "CIGaussianBlur") else { return inverted }
        blurFilter.setValue(inverted, forKey: kCIInputImageKey)
        blurFilter.setValue(2.0, forKey: kCIInputRadiusKey)
        
        return blurFilter.outputImage ?? inverted
    }
    
    /// åº”ç”¨è’™ç‰ˆå¹¶ä¼˜åŒ–ç»“æœ
    private func applyMaskWithOptimization(mask: CIImage, to image: CIImage) -> CIImage {
        // 1. ä½¿ç”¨è’™ç‰ˆæ··åˆ
        guard let blendFilter = CIFilter(name: "CIBlendWithMask") else { return image }
        
        let transparentBackground = CIImage(color: CIColor.clear).cropped(to: image.extent)
        blendFilter.setValue(image, forKey: kCIInputImageKey)
        blendFilter.setValue(transparentBackground, forKey: kCIInputBackgroundImageKey)
        blendFilter.setValue(mask, forKey: kCIInputMaskImageKey)
        
        guard let blended = blendFilter.outputImage else { return image }
        
        // 2. è½»å¾®å¢å¼ºæœ€ç»ˆç»“æœ
        guard let finalFilter = CIFilter(name: "CIColorControls") else { return blended }
        finalFilter.setValue(blended, forKey: kCIInputImageKey)
        finalFilter.setValue(1.05, forKey: kCIInputContrastKey)
        finalFilter.setValue(1.02, forKey: kCIInputSaturationKey)
        
        return finalFilter.outputImage ?? blended
    }
    
    /// iOS 17+ ä½¿ç”¨Visionæ¡†æ¶çš„ä¸»ä½“åˆ†ç¦»åŠŸèƒ½
    @available(iOS 17.0, *)
    private func performVisionSubjectLifting(image: UIImage) async throws -> UIImage {
        print("ğŸ” å¼€å§‹Visionä¸»ä½“åˆ†ç¦»åˆ†æ...")
        
        // æ·»åŠ è¶…æ—¶ä¿æŠ¤
        return try await withThrowingTaskGroup(of: UIImage.self) { group in
            // æ·»åŠ ä¸»è¦ä»»åŠ¡
            group.addTask {
                try await withCheckedThrowingContinuation { continuation in
                    var hasResumed = false
                    let lock = NSLock()
                    
                    // ç¡®ä¿å›¾åƒæœ‰æ•ˆ
                    guard let cgImage = image.cgImage else {
                        continuation.resume(throwing: VisionError.invalidImage)
                        return
                    }
                    
                    // åˆ›å»ºä¸»ä½“åˆ†ç¦»è¯·æ±‚
                    let request = VNGenerateForegroundInstanceMaskRequest { request, error in
                        lock.lock()
                        defer { lock.unlock() }
                        
                        guard !hasResumed else { return }
                        hasResumed = true
                        
                        if let error = error {
                            print("âŒ Visionä¸»ä½“åˆ†ç¦»è¯·æ±‚å¤±è´¥: \(error)")
                            continuation.resume(throwing: error)
                            return
                        }
                        
                        guard let observations = request.results as? [VNInstanceMaskObservation],
                              let observation = observations.first else {
                            print("âš ï¸ æ²¡æœ‰æ£€æµ‹åˆ°ä¸»ä½“")
                            continuation.resume(throwing: VisionError.noResults)
                            return
                        }
                        
                        do {
                            print("âœ… æ£€æµ‹åˆ°ä¸»ä½“ï¼Œå¼€å§‹ç”Ÿæˆè’™ç‰ˆ...")
                            let maskedImage = try self.createMaskedImage(
                                from: image,
                                observation: observation
                            )
                            print("âœ… ä¸»ä½“åˆ†ç¦»å®Œæˆ")
                            continuation.resume(returning: maskedImage)
                        } catch {
                            print("âŒ è’™ç‰ˆç”Ÿæˆå¤±è´¥: \(error)")
                            continuation.resume(throwing: error)
                        }
                    }
                    
                    // åˆ›å»ºè¯·æ±‚å¤„ç†å™¨ï¼Œä¿æŒåŸå§‹å›¾åƒæ–¹å‘
                    let imageOrientation = CGImagePropertyOrientation(image.imageOrientation)
                    let handler = VNImageRequestHandler(
                        cgImage: cgImage,
                        orientation: imageOrientation,
                        options: [:]
                    )
                    
                    // åœ¨åå°é˜Ÿåˆ—æ‰§è¡Œè¯·æ±‚
                    DispatchQueue.global(qos: .userInitiated).async {
                        do {
                            try handler.perform([request])
                        } catch {
                            lock.lock()
                            defer { lock.unlock() }
                            
                            guard !hasResumed else { return }
                            hasResumed = true
                            
                            print("âŒ Visionè¯·æ±‚æ‰§è¡Œå¤±è´¥: \(error)")
                            continuation.resume(throwing: error)
                        }
                    }
                }
            }
            
            // æ·»åŠ è¶…æ—¶ä»»åŠ¡
            group.addTask {
                try await Task.sleep(nanoseconds: 30_000_000_000) // 30ç§’è¶…æ—¶
                throw VisionError.processingTimeout
            }
            
            // è¿”å›ç¬¬ä¸€ä¸ªå®Œæˆçš„ä»»åŠ¡ç»“æœ
            guard let result = try await group.next() else {
                throw VisionError.noResults
            }
            
            group.cancelAll()
            return result
        }
    }
    
    /// ä½¿ç”¨Visionè§‚å¯Ÿç»“æœåˆ›å»ºè’™ç‰ˆå›¾åƒ
    @available(iOS 17.0, *)
    nonisolated private func createMaskedImage(from image: UIImage, observation: VNInstanceMaskObservation) throws -> UIImage {
        // è·å–æ‰€æœ‰å‰æ™¯å®ä¾‹
        let allInstances = observation.allInstances
        
        // ç¡®ä¿æœ‰æ£€æµ‹åˆ°çš„å®ä¾‹
        guard !allInstances.isEmpty else {
            throw VisionError.noResults
        }
        
        // ç›´æ¥ç”Ÿæˆå¸¦è’™ç‰ˆçš„å›¾åƒ
        guard let cgImage = image.cgImage else {
            throw VisionError.invalidImage
        }
        
        do {
            // ä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•ç”Ÿæˆè’™ç‰ˆå›¾åƒ
            let requestHandler = VNImageRequestHandler(cgImage: cgImage, options: [:])
            let maskedPixelBuffer = try observation.generateMaskedImage(
                ofInstances: allInstances,
                from: requestHandler,
                croppedToInstancesExtent: false
            )
            
            // å°†CVPixelBufferè½¬æ¢ä¸ºUIImageï¼Œä¿æŒåŸå§‹æ–¹å‘
            let ciImage = CIImage(cvPixelBuffer: maskedPixelBuffer)
            
            // ä½¿ç”¨ä¸“ç”¨çš„CIContexté¿å…å†…å­˜é—®é¢˜
            let context = CIContext(options: [
                .workingColorSpace: CGColorSpace(name: CGColorSpace.sRGB)!,
                .outputColorSpace: CGColorSpace(name: CGColorSpace.sRGB)!
            ])
            
            guard let outputCGImage = context.createCGImage(ciImage, from: ciImage.extent) else {
                throw VisionError.imageConversionFailed
            }
            
            // ä¿æŒåŸå§‹å›¾åƒçš„æ–¹å‘ä¿¡æ¯
            return UIImage(cgImage: outputCGImage, scale: image.scale, orientation: image.imageOrientation)
        } catch {
            print("âŒ ç”Ÿæˆè’™ç‰ˆå›¾åƒå¤±è´¥: \(error)")
            throw VisionError.filterProcessingFailed
        }
    }
    
    /// ä¿®æ­£å›¾åƒæ–¹å‘
    private func fixImageOrientation(_ image: UIImage) -> UIImage {
        // å¦‚æœå›¾åƒæ–¹å‘å·²ç»æ˜¯.upï¼Œç›´æ¥è¿”å›
        if image.imageOrientation == .up {
            return image
        }
        
        // åˆ›å»ºæ­£ç¡®æ–¹å‘çš„å›¾åƒ
        UIGraphicsBeginImageContextWithOptions(image.size, false, image.scale)
        defer { UIGraphicsEndImageContext() }
        
        image.draw(in: CGRect(origin: .zero, size: image.size))
        
        guard let normalizedImage = UIGraphicsGetImageFromCurrentImageContext() else {
            return image
        }
        
        return normalizedImage
    }
}

// MARK: - æ‰©å±•
extension CGImagePropertyOrientation {
    init(_ uiOrientation: UIImage.Orientation) {
        switch uiOrientation {
        case .up: self = .up
        case .down: self = .down
        case .left: self = .left
        case .right: self = .right
        case .upMirrored: self = .upMirrored
        case .downMirrored: self = .downMirrored
        case .leftMirrored: self = .leftMirrored
        case .rightMirrored: self = .rightMirrored
        @unknown default: self = .up
        }
    }
}

// MARK: - é”™è¯¯å®šä¹‰
enum VisionError: LocalizedError {
    case invalidImage
    case noResults
    case filterCreationFailed
    case filterProcessingFailed
    case imageConversionFailed
    case processingTimeout
    
    var errorDescription: String? {
        switch self {
        case .invalidImage:
            return "æ— æ•ˆçš„å›¾åƒ"
        case .noResults:
            return "æœªèƒ½æ£€æµ‹åˆ°ä¸»ä½“"
        case .filterCreationFailed:
            return "æ»¤é•œåˆ›å»ºå¤±è´¥"
        case .filterProcessingFailed:
            return "å›¾åƒå¤„ç†å¤±è´¥"
        case .imageConversionFailed:
            return "å›¾åƒè½¬æ¢å¤±è´¥"
        case .processingTimeout:
            return "å¤„ç†è¶…æ—¶"
        }
    }
} 